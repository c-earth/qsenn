\documentclass[preprint, 12pt]{revtex4-2}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator{\Tr}{Tr}
\usepackage{physics}
\usepackage{xcolor}

\def\thesection{\arabic{section}}
\def\thesubsection{\arabic{section}.\arabic{subsection}}
\def\thesubsubsection{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
\numberwithin{equation}{section}

\begin{document}

\title{Unified Representation for Quantum Spin Equivariant Learning}

\author{Abhijatmedhi Chotrattanapituk}
\affiliation{Quantum Measurement Group, MIT, Cambridge, MA, USA \\
            Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA, USA}

\date{\today}

\begin{abstract}
    TBA
\end{abstract}

\maketitle
\newpage

\section{Introduction}

\section{Symmetries in Quantum Spin Systems}

\newpage
\section{Notations}
Due to the number of groups, and representations discussed in this work, it is important to define some common definitions and notations that would be used. 

\subsection{Group}
A group is a set of items together with a binary operator defined on each pair of elements in the group such that, for an arbritary group of set $G$ and its binary operator $\circ_G$, the following properites must be true.
\begin{enumerate}
    \item For any $g_1, g_2\in G$,
        \begin{equation}\label{eq:group_closed}
            g_1\circ_G g_2\in G.
        \end{equation}
    \item For any $g_1, g_2, g_3\in G$, 
        \begin{equation}\label{eq:group_associative}
            (g_1\circ_G g_2)\circ_G g_3 = g_1\circ_G(g_2\circ_G g_3).
        \end{equation}
    \item There is a unique element $e_G\in G$ such that, for any $g\in G$,
        \begin{equation}\label{eq:group_identity}
            g\circ_G e_G=e_G\circ_G g=g.
        \end{equation}
    \item For any $g\in G$, there is a unique element $g^{-1}\in G$ such that 
        \begin{equation}\label{eq:group_inverse}
            g\circ_G g^{-1}=g^{-1}\circ_G g=e.
        \end{equation}
\end{enumerate}

For compactness in the remaining sections, we would drop the explicit placement of the binary operator and assume the group binary operation whenever there are two elements of the same group placeing beside each other, e.g., if $g_1, g_2\in G$, $g_1g_2\triangleq g_1\circ_G g_2$. Furthermore, we would refer to the group by its set when ever the group's binary operator is unambiguous, e.g., a group of set $G$ and its binary operator $\circ_G$ is group $G$.

\subsection{Homomorphism}
A homomorphism of group $G$ is a map from $G$ to another group $H$ such that the image of $G$ preserves the same structure as $G$. In otherwords, for a homomorphism $h:G\rightarrow H$,
\begin{equation}\label{eq:homomorphism}
    \forall g_1, g_2\in G,\, h(g_1)h(g_2) = h(g_1g_2),
\end{equation}
where the implicit binary operation $g_1g_2$ is from $G$, i.e. $g_1\circ_G g_2$ while implicit binary operation $h(g_1)h(g_2)$ is from $H$, i.e. $h(g_1)\circ_H h(g_2)$.

If a homomorphism is bijective, i.e. each and every element in $H$ is mapped from an element in $G$ homomorphically. The map is called isomorphism, and it entails equivalence relation between isomorphic groups. We denoted group isomorphic equivalence between $G$, and $H$ with $G\cong H$. 

\subsection{Representation}
A group representation of $G$ on vector space $V$ is a group homomorphism from $G$ to general linear group $GL(V)$ which is the group of bijective linear transformation from $V$ to itself. To be more specific, for the vector space $V$ over field $\mathbb{F}_V$ with $n_V$ dimensions, $GL(n_V, \mathbb{F}_V)\triangleq GL(V)$ is the set of $n_V$ by $n_V$ invertible matrices over $\mathbb{F}_V$ with matrix multiplication as the group binary operator. This obviously means that there can be multiple representations depending on the choice of $V$, and each $V$ might has multiple representations as well. 

Assuming that we are able to uniquely name each representation, we would use the notation $D_{G}^{(\Gamma)}:G\rightarrow GL$ for a representation of group $G$ where $\Gamma$ is the label that fully specify the Rep. Since two representations can be equivalence if every corresponding pair of matrix representing group elements are equivalence through the same basis transformation, we would decorate $D_{G}^{(\Gamma)}$ to specify the basis of the matrix representation used.

\subsection{Reducibility of Representation and Irreducible Representation}
A group representation of $G$ on vector space $V$ is reducible if their exist an invariant proper subspace $W\subsetneq V$ such that $W$ is closed under group represented action, i.e.
\begin{equation}\label{eq:invariant subspace}
    \forall g\in G,\,\forall w\in W,\, D_{G}^{(\Gamma)}(g)w\in W.
\end{equation}
This reducibility implies that there exists a choice of basis that block diagonalizes group representation such that one of the blocks is a (reduced) group representation of $G$ on vector space $W$.

A group irreducible representation of $G$ on vector space $V$ is a group representation that is not reducible, i.e. cannot be block diagonalized.

\subsection{Tensor Product}
Tensor product is an operation to combine vector spaces. It is generally denoted by a bilinear map $U\times V\rightarrow U\otimes V$ such that for any basis vector $\hat{e}_U\in U$, and basis vector $\hat{e}_V\in V$ their tensor product gives a basis vector $\hat{e}_U\otimes \hat{e}_V$. This means that, for any vector $u\in U$, and $v\in V$,
\begin{equation}\label{eq:vector tensor product}
    u\otimes v = \left(\sum_{a}u_a\hat{e}_U^a\right)\otimes\left(\sum_{b}v_b\hat{e}_V^b\right) = \sum_{a,b}u_av_b\hat{e}_U^a\otimes\hat{e}_V^b.
\end{equation}
From this, one can extend the concept to linear maps of tensor product space. Consider two linear maps $S:U\rightarrow U$, and $T:V\rightarrow V$, the tensor product of these two maps is defined as $S\otimes T:U\otimes V\rightarrow U\otimes V$ such that, for any $u\otimes v\in U\otimes V$,
\begin{equation}\label{eq:linear map tensor product}
    (S\otimes T)(u\otimes v) = S(u)\otimes T(v).
\end{equation}
From equation \ref{eq:linear map tensor product}, we can also directly impliy that, if there are $Q:U\rightarrow U$, and $R:V\rightarrow V$,
\begin{equation}\label{eq:compose linear map tensor product}
    (Q\otimes R)(S\otimes T) = QS\otimes RT
\end{equation}

\subsection{Kronecker Product}
Kronecker product is a specialization of tensor product for matrices. Given matrices $M$, and $N$, their Kronecker product is defined as
\begin{equation}\label{eq:Kronecker product}
    \left[M\otimes_KN\right]_{ac,bd} \triangleq \left[M\right]_{a,b}\cdot\left[N\right]_{c,d}.
\end{equation}
Here, the subscripts of the $M\otimes_KN$, i.e. $ac$, and $bd$, are two-digit indices not multiplication. In other words, the resulting product has dimensions of the product of the corresponding dimensions of the compounding matrices, i.e. if $M$ is a $h_M$ by $w_M$ matrix, and $N$ is a $h_N$ by $w_N$ matrix, then $M\otimes_KN$ is a $h_M\cdot h_N$ by $w_M\cdot w_N$ matrix.

Since we would mostly utilize matrix in the form of general linear representation, consider $M\in GL(U)$, and $N\in GL(V)$ with dimensions $n_U$, and $n_V$ respoectively. Because both of them have inverses, we can define
\begin{equation}\label{eq:inverse Kronecker product}
    \left[(M\otimes_KN)^{-1}\right]_{ac,bd} \triangleq \left[M^{-1}\otimes_KN^{-1}\right]_{ac,bd} = \left[M^{-1}\right]_{a,b}\cdot\left[N^{-1}\right]_{c,d}.
\end{equation}
Then,
\begin{equation}\label{eq:Kronecker product GL identity}
    \begin{aligned}
        \left[(M\otimes_KN)(M^{-1}\otimes_KN^{-1})\right]_{ac,ef} &= \sum_{b,d}\left[M\right]_{a,b}\cdot\left[N\right]_{c,d}\cdot\left[M^{-1}\right]_{b,e}\cdot\left[N^{-1}\right]_{d,f} \\
        \left[MM^{-1}\otimes_KNN^{-1}\right]_{ac,ef} &= \left(\sum_{b}\left[M\right]_{a,b}\cdot\left[M^{-1}\right]_{b,e}\right)\left(\sum_{d}\left[N\right]_{c,d}\cdot\left[N^{-1}\right]_{d,f}\right) \\
        \left[I_U\otimes_KI_V\right]_{ac,ef} &= \left[MM^{-1}\right]_{a,e}\cdot\left[NN^{-1}\right]_{c,f} = \left[I_U\right]_{a,e}\cdot\left[I_V\right]_{c,f}
    \end{aligned}
\end{equation}
where $I_U$, and $I_V$ are identity matrices of $U$, and $V$ respectively. This results imply that Kronecker product between invertible matrices of $n_U$, and $n_V$ dimensions is an invertible matrix of $n_U\cdot n_V$ dimensions with matrices and its inverses definded by equation \ref{eq:Kronecker product}, and \ref{eq:inverse Kronecker product}, identity defined by the end of equation \ref{eq:Kronecker product GL identity}, and binary operator defined by equation \ref{eq:compose linear map tensor product}. In other words, Kronecker product between general linear groups of two vector spaces $U$, and $V$ is homomorphic to general linear group of $U\otimes V$.

\subsection{Groups' Direct Product}
A group direct product is a composition of two groups to form a new larger group. Consider groups $G$ and $H$, their direct product is defined as a group of Cartesian product set
\begin{equation}\label{eq:Cartesian product}
    G\times H = \{(g, h)|\forall g\in G,\, \forall h\in H\},
\end{equation}
and its binary operator defined as, $\forall (g_1, h_1), (g_2, h_2)\in G\times H$,
\begin{equation}\label{eq:direct product binary operator}
    (g_1, h_1)(g_2, h_2) = (g_1g_2, h_1h_2).
\end{equation}
Here, we also utilize the implicit binary operation notations.

Consider a pair of representations $D^{(\Gamma_G)}_G$, and $D^{(\Gamma_H)}_H$ of $G$, and $H$ respectively. One can define a representation of $G\times H$ with
\begin{equation}\label{eq:direct product representation}
    D^{(\Gamma_G,\Gamma_H)}_{G\times H} = D^{(\Gamma_G)}_G\otimes_K D^{(\Gamma_H)}_H
\end{equation}
because the structure of groups' direct product (equation \ref{eq:direct product binary operator}) is the same as general linear groups' tensor product (equation \ref{eq:compose linear map tensor product}).

% Leave it at this for now, but might come back for self-sustaining work
\textcolor{red}{
Note that the representation of direct product defined in equation \ref{eq:direct product representation} guarantees irreducible representation from product of two irreducible representation, and complete identification of all irreducible representations of the groups' direct product as long as the groups are either finite, or compact.}

\subsection{Free Product}
A group free product is another composition of two groups to form a new larger group. Consider groups $G$ and $H$, their free product ($G\ast H$) is defined as a group generated from $G\cup H$ such that the binary operator is $\circ_G$ between adjacent elements from $G$, and is $\circ_H$ between adjacent elements from $H$,
\begin{equation}\label{eq:free product}
    G\ast H = \bigcup_{N}(G\cup H)^N.
\end{equation}
In other words, $G\ast H$ is a group of arbritary length of words where each words' character is chosen from $G\cup H$. Since $G$, and $H$ are groups, all adjacent elements from $G$ (or $H$) can be replaced by an element from $G$ (or $H$). Hence, $G\ast H$ is actually a group of arbritary length of words where each words' character is alternately chosen from $G$, and $H$. From here, one can specialize free product according to the relation between the compounding groups.

The fact that we are discussing free product of groups is because the operators that apply symmetric transformation from different symmetry groups act on the same system, i.e. vector space. This means that all those operators are actually in the free product group of all considered symmetry group.

\subsection{Free Product of two Commuting Groups}
From previous section, if we also add that $G$, and $H$ are commuting subgroups of $G\ast H$, i.e. $\forall g\in G.\forall h\in H$, $gh=hg$, we can  futher specialize $G\ast H$ by commuting all elements from $G$ to the front of each word (all elements from $H$ are on the backside of each word,) and replacing all adjacent elements from $G$ (or $H$) with and element from $G$ (or $H$). In other words, if $G$, and $H$ commute with each other,
\begin{equation}\label{eq:commute free product}
    G\ast H = \{gh|g\in G, h\in H\}.
\end{equation}
Consider two elements $g_1h_1, g_2h_2\in G\ast H$ where $g_1, g_2\in G$, and $h_1, h_2\in H$,
\begin{equation}\label{eq:commute free product binary operator}
    (g_1h_1)\circ_{G\ast H} (g_2h_2) = g_1h_1g_2h_2=g_1g_2h_1h_2=(g_1\circ_Gg_2)(h_1\circ_Hh_2).
\end{equation}
From free product of commuting groups (equation \ref{eq:commute free product}, and \ref{eq:commute free product binary operator}) and direct product (equation \ref{eq:Cartesian product}, and \ref{eq:direct product binary operator}), we can directly make an isomorphism that maps $gh\in G\ast H$ to $(g,h)\in G\times H$ for every $g\in G$, and $h\in H$. 

This means that $G\ast H\cong G\times H$, and all irreducible representation for free product of commuting groups can be constructed from irreducible representations of the compounding groups according to equation \ref{eq:direct product representation}. This result is very important when we are describing a physical system that contain multiple symmetries which are all commute with each other.

% Leave it at this for now, but might come back for self-sustaining work
\subsection{Reduction of Representation's Product}

\newpage
\section{$SU(2)$ Group}

\subsection{$SU(2)$'s Physical Properites}

\subsection{$SU(2)$'s Definition}
$SU(2)$, i.e. special unitary group of dimension 2, is the group of 2 by 2 unitary matrices with unit determinant with matrix multiplication as its binary operator. In other words,
\begin{equation}\label{eq:SU2}
    SU(2) = \{u \in GL(2, \mathbb{C})|u^\dagger=u^{-1}, \det(u) = 1\},
\end{equation}    
where $u^\dagger$ is the complex conjugate transposition of $u$, and $\det(\cdot)$ is matrix determinant operation. Since, for any $u, v \in SU(2)$,
\begin{equation}\label{eq:determinant identity}
    \det(uv) = \det(u)\det(v) = 1,
\end{equation}
and
\begin{equation}\label{eq:unitary closed}
    (uv)^\dagger = (uv)^{\top\ast} = (v^\top u^\top)^\ast = v^\dagger u^\dagger = v^{-1}u^{-1} = (uv)^{-1},
\end{equation}
$SU(2)$ is a closed subgroup of $GL(2, \mathbb{C})$ which makes it a Lie group. This means that there is a corresponding Lie algebra, $\mathfrak{su(2)}$, such that 
\begin{equation}\label{eq:SU2 Lie group}
    SU(2) = \{e^g|g \in \mathfrak{su(2)}\}
\end{equation}
where the exponential of a square matrix $g$ is defined as
\begin{equation}\label{eq:matrix exponential}
    e^g = I + \sum_{k=1}^{\infty}\dfrac{g^k}{k!}.
\end{equation}
Here, $I$ representes identity matrix with the same shape as $g$. From definition of matrix exponential,
\begin{equation}\label{eq:matrix exponential properties}
    \begin{aligned}
        (e^g)^\dagger &= e^{g^\dagger}, \\
        (e^g)^{-1} &= e^{-g}, \\
        \det(e^g) &= e^{\Tr g},
    \end{aligned}
\end{equation}
one can define the Lie algebra of $SU(2)$ as
\begin{equation}\label{eq:su2}
    \mathfrak{su(2)} = \{g \in M(2, \mathbb{C})|g^\dagger=-g, \Tr(g) = 0\},
\end{equation}
where $M(2,\mathbb{C})$ is the group of 2 by 2 matrices with complex field with matrix multiplication as its binary operator, and $\Tr(\cdot)$ is matrix trace operation. Hence, each member of $\mathfrak{su(2)}$ can be written in the form
\begin{equation}\label{eq:su2 polar angle form}
    \begin{bmatrix}
        iv_z & v_y+iv_x \\
        -v_y+iv_x & -iv_z
    \end{bmatrix}
      = iv_x\sigma_x + iv_y\sigma_y + iv_z\sigma_z = i\vec{v}\cdot\vec{\sigma}
\end{equation}
where $\vec{v} \in \mathbb{R}^3$, $\vec{\sigma}=\sigma_x\hat{x}+\sigma_y\hat{y}+\sigma_z\hat{z}$, and $\sigma_i$'s are Pauli matrices defined as
\begin{equation}\label{eq:Pauli's matrices}
    \sigma_x = \begin{bmatrix}
                    0 & 1 \\
                    1 & 0
                \end{bmatrix},
    \sigma_y = \begin{bmatrix}
                    0 & -i \\
                    i & 0
                \end{bmatrix},
    \sigma_z = \begin{bmatrix}
                    1 & 0 \\
                    0 & -1
                \end{bmatrix}.
\end{equation}

\subsection{$SU(2)$'s Generators}
From $\mathfrak{su(2)}$'s definition in the previous section, one can consider using Pauli matrices as the group generators for $SU(2)$ according to equation \ref{eq:su2 polar angle form}, but it is more common to use $J^{(1/2)}_i \triangleq \sigma_i/2$ as the generators where $i\in \{x, y, z\}$, and $\vec{\phi}\triangleq-2\vec{v}\in\mathbb{R}^3$ as group parameterization. In other words, we can redefine $SU(2)$ as
\begin{equation}\label{eq:SU2 parameterization}
    SU(2) = \{u_{\vec{\phi}}|u_{\vec{\phi}}=e^{-i\vec{J}^{(1/2)}\cdot\vec{\phi}},\, \vec{\phi}\in\mathbb{R}^3\},
\end{equation}
with $\vec{J}^{(1/2)}=J_x^{(1/2)}\hat{x}+J_y^{(1/2)}\hat{y}+J_z^{(1/2)}\hat{z}$. This parameterization of $SU(2)$ group directly implies that
\begin{equation}\label{eq:SU2 parameterization elements}
    \begin{aligned}
        e_{SU(2)} &= u_{\vec{0}}, \\
        u_{\vec{\phi}}^{-1} &= u_{-\vec{\phi}}.
    \end{aligned}
\end{equation}

With this choice of generators, it gives the Lie bracket (commutation) relation
\begin{equation} \label{eq:su2 bracket}
    \left[J^{(1/2)}_i, J^{(1/2)}_j\right] \triangleq J^{(1/2)}_iJ^{(1/2)}_j-J^{(1/2)}_jJ^{(1/2)}_i = i\epsilon_{ijk}J^{(1/2)}_k
\end{equation}
where $\epsilon_{ijk}$ is equal to 1 if $(i, j, k)$ is an even permutation of $(x, y, z)$, $-1$ if odd permutation, and 0 otherwise.

The next step is to consider an arbritary representation of $SU(2)$ such that its generators, represented by $J_i$'s, obey equation \ref{eq:su2 bracket}. With out loss of generality, consider the representation vector space with eigenvectors of $J_z$, $\{\ket{\lambda_{J_z}}\}$, as the basis,
\begin{equation}\label{eq:Jz eigenvalue equation}
    J_z\ket{\lambda_{J_z}}=\lambda_{J_z}\ket{\lambda_{J_z}}.
\end{equation}
Furthermore, we will also assume that this representation is irreducible (proof of this assumption will be in the following section.) With the standard method, first define two additional operators
\begin{equation}\label{eq:ladder operators}
    \begin{aligned}
        J_+ &= J_x + iJ_y, \\
        J_- &= J_x - iJ_y.
    \end{aligned}
\end{equation}
From Lie bracket, and the choice of basis used,
\begin{equation}\label{eq:ladder equation}
    \begin{aligned}
        J_zJ_+\ket{\lambda_{J_z}} &= J_+(J_z+1)\ket{\lambda_{J_z}} = (\lambda_{J_z}+1)J_+\ket{\lambda_{J_z}}, \\
        J_zJ_-\ket{\lambda_{J_z}} &= J_-(J_z-1)\ket{\lambda_{J_z}} = (\lambda_{J_z}-1)J_-\ket{\lambda_{J_z}}.
    \end{aligned}
\end{equation}
This directly implies
\begin{equation}\label{eq:ladder_propto}
    \begin{aligned}
        J_+\ket{\lambda_{J_z}} &\propto \ket{\lambda_{J_z}+1}, \\
        J_-\ket{\lambda_{J_z}} &\propto \ket{\lambda_{J_z}-1}.
    \end{aligned}
\end{equation}

Before proceeding, we need to clarify an assumption we made to get equation \ref{eq:ladder_propto} that there is no degeneracy of $\lambda_{J_z}$ in the basis. This result is directly entailed from the assumption that the representation is irreducible. To prove this, first assume that there exist $\ket{\lambda_{J_z}}'$ that has the same eigenvalue as $\ket{\lambda_{J_z}}$. From the properties of $J_i$'s, we can choose the representation such that equation \ref{eq:ladder_propto} is true, i.e. the operators never create primed vector from unprimed vector. Since the representation of each $SU(2)$ member can be expressed in terms of the generators (equation \ref{eq:SU2 Lie group} and \ref{eq:matrix exponential}), if a subspace is invariant under the generators, it is also invariant under the $SU(2)$ representation. This means that the proper subspace that exclude $\ket{\lambda_{J_z}}'$ is invariant for this choice of representation. Hence, the representation is reducible which contradict the assumption.

To get the proportionality, consider another operators
\begin{equation}\label{eq:J^2}
    J^2 \triangleq J_x^2 + J_y^2+J_z^2.
\end{equation}
One can directly chack that the new operator commutes with all $J$'s mentioned so far. Furthermore, it is straight forward that
\begin{equation}\label{eq:J^2 identity}
    \begin{aligned}
        J^2 &= J_+J_- + J_z^2 - J_z \\
        &= J_-J_+ + J_z^2 + J_z.
    \end{aligned}
\end{equation}
Since $J^2$ commutes with $J_z$, the basis used are also eigenbasis of $J^2$ with some eigenvalue $\lambda_{J^2}$. Hence, we can directly relabel the eigenbasis from $\ket{\lambda_{J_z}}$ to $\ket{\lambda_{J^2}, \lambda_{J_z}}$, and get
\begin{equation}\label{eq:ladder norm square}
    \begin{aligned}
        \mel{\lambda_{J^2}, \lambda_{J_z}}{J_+J_-}{\lambda_{J^2}, \lambda_{J_z}} &= \lambda_{J^2} - \lambda_{J_z}(\lambda_{J_z} - 1), \\
        \mel{\lambda_{J^2}, \lambda_{J_z}}{J_-J_+}{\lambda_{J^2}, \lambda_{J_z}} &= \lambda_{J^2} - \lambda_{J_z}(\lambda_{J_z} + 1).
    \end{aligned}
\end{equation}
Since the norm square of any vector in vector space must be non-negative, this requires
\begin{equation}\label{eq:eigenvalues' condition}
    \lambda_{J^2} \geq \abs{\lambda_{J_z}}(\abs{\lambda_{J_z}}+1) \geq 0.
\end{equation}
However, the $J_\pm$ makes $\lambda_{J_z}$ unbounded unless the vector vanished at some points, i.e. the equality must hold for the upper, and lower bounds of the possible $\lambda_z$'s. If we replace $\lambda_{J^2}$ with $j(j+1)$ where $j$ non-negative. The requirement makes $\lambda_{J_z, \max} = j$, and $\lambda_{J_z, \min} = -j$. Since any pair of $\lambda_{J_z}$'s are different by an integer, 
\begin{equation}\label{eq:eigenvalues' requirement}
    \lambda_{J_z, \max} - \lambda_{J_z, \min} = 2j \in \mathbb{Z}_0^+.
\end{equation}
Hence, we can relabel the eigenbasis from $\ket{\lambda_{J^2}, \lambda_{J_z}}$ to $\ket{j, m}$ and choose the proportionality such that
\begin{equation} \label{eq:J ladder}
    \begin{aligned}
        J_+\ket{j, m} &= \sqrt{j(j+1) - m(m + 1)}\ket{j, m+1}, \\
        J_-\ket{j, m} &= \sqrt{j(j+1) - m(m - 1)}\ket{j, m-1},
    \end{aligned}
\end{equation}
where $j \in \{0, 1/2, 1, 3/2, ...\}$, and $m \in \{-j, -j+1, ..., j-1, j\}$. Next, we must prove that each $j$ fully identifies an irreducible representation. Assuming that this is not the case by letting there be basis vector $\ket{j', m'}$ in the vector space of this irreducible representation that is not linearly dependent on the eigenbasis $\{\ket{j,m}\}$, equation \ref{eq:J ladder} makes the vector space that exclude $\ket{j', m'}$ invariant proper subspace (with reasoning similar to when we prove non-degeneracy of $\lambda_{J_z}$). Hence, this implies the reducibility of the representation which contradicts the assumption. 

\subsection{$SU(2)$'s Irreducible Representations}
In previous section, we have proved that every irreducible representation of $SU(2)$ has its unique $j$ (no irreducible representation requires basis of more than one $j$.) Furthermore, since we have not made any other assumption, $j$ also completely identify every irreducible representation of $SU(2)$ (representations identified by $j$'s exhaust all possible irreducible representations.) In other words, each and every irreducible representation can be uniquely labeled by $j \in \{0, 1/2, 1, 3/2, ...\}$.

Last thing we need to do for that completion of $SU(2)$'s irreducible representation is that the representations labeled by $j$'s (previously assumed to be irreducible) is actually irreducible. Consider a representation of $SU(2)$ that is generated by generators  $J^{(j)}_x$, $J^{(j)}_y$, and $J^{(j)}_z$ where the superscripts indicate that the representation is labeled by $j$. Assuming that this representation is reducible, there must be a unitary basis transformation that nontrivial block diagonalize all elements of $SU(2)$ in this representation. From equation \ref{eq:matrix exponential} and \ref{eq:SU2 parameterization}, the representation labeled by $j$ has, for $\vec{\phi}\in\mathbb{R}^3$,
\begin{equation}\label{eq:SU2 element parameterization}
    u_{\vec{\phi}} = e^{-i\vec{J}^{(j)}\cdot\vec{\phi}} = I+\sum_{k=1}^{\infty}(-i\vec{J}^{(j)}\cdot\vec{\phi})^k,
\end{equation}
as its parameterized matrix representation. Since $\vec{\phi}$ is arbritary, the block diagonalization must holds for every term on the right hand side of equation \ref{eq:SU2 element parameterization}. This implies simultaneous block diagonalization of $J^{(j)}_x$, $J^{(j)}_y$, and $J^{(j)}_z$. In other words, there is an invariant proper subspace, say $W$, that closed under $J^{(j)}_+$, and $J^{(j)}_-$. Since such vector space is a proper subspace, there must exist $-j\leq m'\leq j$ such that the projection of $\ket{j, m'}$ on this space is not $\ket{j, m'}$, i.e. there is nontrivial orthogonal residual. Also, there must exist $-j\leq m''\leq j$ such that the projection of $\ket{j, m''}$ on this space is not zero, i.e. there is nontrivial projection. Let the projection of $\ket{j, m''}$ on to $W$ be
\begin{equation}\label{eq:SU2 projection}
    \text{Proj}_W(\ket{j, m''}) = \sum_{m = -j}^{j}C''_m\ket{j,m}.
\end{equation}
Let the smallest $m$ that $C''_m\neq 0$ be $m^*$. This means that applying $J^{(j)}_+$ for $j-m^*$ times gives nonzero proportional to $\ket{j,j}$, i.e.
\begin{equation}\label{eq:projection extremum}
    \left[J^{(j)}_+\right]^{j-m^*}\text{Proj}_W(\ket{j, m''}) \propto \ket{j, j}.
\end{equation}
After that, apply $J^{(j)}_-$ for $j-m'$ times would also give nonzero proportional to $\ket{j,m'}$, i.e.
\begin{equation}\label{eq:projection to orthogonal}
    \left[J^{(j)}_-\right]^{j-m'}\left[J^{(j)}_+\right]^{j-m^*}\text{Proj}_W(\ket{j, m''}) \propto \ket{j, m'}.
\end{equation}
However, since $\text{Proj}_W(\ket{j, m''})\in W$ and $\ket{j, m'}-\text{Proj}_W(\ket{j, m'})\neq 0$, $W$ is not closed under $J^{(j)}_+$, and $J^{(j)}_-$ which gives contradict. Therefore, the representations we have constructed previously are actually irreducible.

From computational perspective, in order to implement the representation, the basis of the space in which all group operations would act on need to be carefully selected mainly for the purpose of reducing computational costs. However, it requires the consideration of every symmetry group we want to included. Hence, for now, we are assuming the standard $\{\ket{j, m}\}$ basis. In this basis, the irreducible representation labeled with $j$ is the $2j+1$ dimensional space representation. Let the representation of a vector is in the ascending order of the basis from $m = -j$ to $m = j$. We will also, through the rest of this work, index matrices with python-like notation, i.e. indices count from zero. Hence, we can write down
\begin{equation}\label{eq:J's standard basis}
    \begin{aligned}
        \left[J_z^{(j)}\right]_{a,b} &= (a-j)\cdot\delta_{a, b}, \\
        \left[J_+^{(j)}\right]_{a,b} &= \sqrt{j(j+1)-(a-j)(b-j)}\cdot\delta_{a,b+1}, \\
        J_-^{(j)} &= J_+^{(j)\top}, \\
        J_x^{(j)} &= \left(J_+^{(j)}+J_-^{(j)}\right)/2, \\
        J_y^{(j)} &= -i\left(J_+^{(j)}-J_-^{(j)}\right)/2,
    \end{aligned}
\end{equation}
where $a, b\in \{0, 1, ..., 2j\}$, $j\in \{0, 1/2, 1, 3/2, ...\}$, and $\delta$ is Kronecker delta. Therefore, we can also write down the irreducible representation labeled by $j$ of $u_{\vec{\phi}}\in SU(2)$ with elements parameterized by $\vec{\phi}\in\mathbb{R}^3$ in this basis as
\begin{equation}\label{eq:SU2 Irreps}
    D^{(j)}_{SU(2)}(u_{\vec{\phi}}) = \exp(-i\vec{J}^{(j)}\cdot\vec{\phi}).
\end{equation}

\newpage
\section{$P$ (Parity) Group}

\subsection{$P$'s Definition}
$P$ is the group of spatial inversion, i.e. the group consisting of only 2 elements. The first is its unique identity $e_P$, and the second is the parity operator $\pi$ (which is its own inverse) that inverts spatial state. The only binary operation rules of this group are
\begin{equation}\label{eq:P binary operations}
    \begin{aligned}
        e_Pe_P = \pi\pi &= e_P, \\
        \pi e_P = e_P \pi &= \pi.
    \end{aligned}
\end{equation}

\subsection{$P$'s Representations}
From equation \ref{eq:P binary operations}, and the fact that there are only 2 elements in the group, any $n$ dimensional representation of $P$ must be the identity matrix for $e_P$, and a self-inverse matrix for $\pi$. Since they are both invertible, it is possible to do eigenvalue decomposition of both of them such that
\begin{equation}\label{eq:P eigen decompose}
    \begin{aligned}
        D_P(e_P) &= UIU^\dagger, \\
        D_P(\pi) &= U\Sigma U^\dagger.
    \end{aligned}
\end{equation}
where $I$ is identity matrix, $\Sigma$ is a diagonal matrix of $D_P(\pi)$'s eigenvalues, and $U$ is the unitary matrix where each column is an eigenvector corresponding to each eigenvalue in $\Sigma$. From this, it is straight forward that all $n$ dimensional representations of $P$ are reducible to 1 dimensional representations.

\subsection{$P$'s Irreducible Representations}
Since 1 dimensional representations cannot be reduced (there is no proper subspace,) all 1 dimensional representations are irreducible representations. From previous section, we now know that all irreducible representations of $P$ must be 1 dimension representation. Furthermore, since $D_P(\pi)$ is self-inverse, $[D_P(\pi)]^2= D_P(e_P)=1$. Hence, there are only two irriducible representations commonly denoted as even ($D_P(\pi)= 1$), and odd ($D_P(\pi)=-1$). 

We can see that the only difference of these two irreducible representations is in the values of $\pi$'s representations. Therefore, we can directly use those values as the irreducible representations' labels as
\begin{equation}\label{eq:P Irreps}
    \begin{aligned}
        D_P^{(p)}(e_P) &= 1 \\
        D_P^{(p)}(\pi) &= p
    \end{aligned}
\end{equation}
where the superscripts indicate that the representation is labeled by $p\in \{-1, 1\}$.

\subsection{$SU(2)\times P$'s Irreducible Representations}

\newpage
\section{$T$ (Time-Reversal) Group}

\subsection{$T$'s Definition}
$T$ is the group of temporal inversion, i.e. the group consisting of its unique identity $e_T$, and the time-reverse operator $\theta$ (which may or may not be its own inverse) that inverts direction of time. However, even with some similarities, this group is not isomorphic with $P$, and the reason is that the physical properties require time-reversal symmetric system to obey Schr\"odinger's equation. To see this, first consider the Schr\"odinger's equation describing an arbritary system,
\begin{equation}\label{eq:Schrodinger}
    i\hbar\dfrac{\partial}{\partial t}\ket{\psi(t)} = \hat{H}\ket{\psi(t)}
\end{equation}

\subsection{$TR$'s Irreducible Representation}
In corporateing anti-unitary group $TR$ with $SU(2)\times P$ cannot be done in the same fashion as combinding $P$ with $SU(2)$ since the anti-unitary nature makes it impossible to write down common eigenbasis even if $\theta$ commutes with every element in $SU(2)\times P$.

To proceed from this point, we need to consider the effects of $\theta$ on the basis chosen for $SU(2)\times P$'s implementation. Since the multiplication of two anti-unitary operators is a unitary operator the irreducible representation of $w\in SU(2)\times P$ in $\{\theta\ket{j, m, p}\}$ basis is
\begin{equation}
    \tilde{D}_{SU(2)\times P}(w) = D_{SU(2)\times P}(\theta^{-1}w\theta)^\ast.
\end{equation}
However, similar to $P$'s case, the system in which the $SU(2)\times P$ describes actually makes $\theta$ commutes with every element in $SU(2)\times P$. This means that 
\begin{equation}
    D_{SU(2)\times P}(\theta^{-1}w\theta)^\ast = D_{SU(2)\times P}(w)^\ast,
\end{equation}
but we can always write
\begin{equation}
    D_{SU(2)\times P}(w) = \pm\exp(i\vec{J}\cdot\vec{\phi}) = \pm\exp(i(J_x\phi_x+J_y\phi_y+J_z\phi_z)).
\end{equation}
This means that
\begin{equation}
    D_{SU(2)\times P}(w)^\ast = \pm\exp(-i(J_x\phi_x-J_y\phi_y+J_z\phi_z)).
\end{equation}
Then, consider a matrix
\begin{equation}
    \left[U^{(j)}\right]_{qr} = i^{2(q-j)}\delta_{q,2j-r}.
\end{equation}
This matrix is a unitary matrix since
\begin{equation}
    \begin{aligned}
        \left[U^{(j)\dagger}\right]_{qr} &= (-i)^{2(r-j)}\delta_{r,2j-q}, \\
        \left[U^{(j)\dagger}U^{(j)}\right]_{qr} &= \sum_s(-i)^{2(s-j)}\delta_{s,2j-q}i^{2(s-j)}\delta_{s,2j-r} \\
        &= \sum_s\delta_{s,2j-q}\delta_{s,2j-r} = \delta_{q,r}, \\
        \left[U^{(j)}U^{(j)\dagger}\right]_{qr} &= \sum_si^{2(q-j)}\delta_{q,2j-s}(-i)^{2(r-j)}\delta_{r,2j-s} \\
        &= \sum_si^{2(q-r)}\delta_{q,2j-s}\delta_{r,2j-s} = i^{2(q-r)}\delta_{q,r} = \delta_{q,r}.
    \end{aligned}
\end{equation}
From this matrix, we can see that
\begin{equation}
    \begin{aligned}
        \left[U^{(j)\dagger}J_z^{(j)}U^{(j)}\right]_{qr} &= \sum_{s,t}(-i)^{2(s-j)}\delta_{s,2j-q}(s-j)\delta_{s,t}i^{2(t-j)}\delta_{t,2j-r} \\
        &= \sum_{s}(-i)^{2(s-j)}\delta_{s,2j-q}(s-j)\delta_{s,2j-r}i^{2(j-r)} \\
        &= (-i)^{2(j-q)}(j-q)\delta_{q, r}i^{2(j-r)} \\
        &= -(q-j)\delta(q,r) \\
        &= \left[-J_z^{(j)}\right]_{qr},
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \left[U^{(j)\dagger}J_+^{(j)}U^{(j)}\right]_{qr} &= \sum_{s,t}(-i)^{2(s-j)}\delta_{s,2j-q}\sqrt{j(j+1)-(s-j)(t-j)}\delta_{s,t+1}i^{2(t-j)}\delta_{t,2j-r} \\
        &= \sum_{s}(-i)^{2(s-j)}\delta_{s,2j-q}\sqrt{j(j+1)-(s-j)(j-r)}\delta_{s,2j-r+1}i^{2(j-r)} \\
        &= (-i)^{2(j-q)}\sqrt{j(j+1)-(j-q)(j-r)}\delta_{2j-q,2j-r+1}i^{2(j-r)} \\
        &= -\sqrt{j(j+1)-(q-j)(r-j)}\delta_{q+1,r} \\
        &= \left[-J_+^{(j)\top}\right]_{qr} = \left[-J_-^{(j)}\right]_{qr},
    \end{aligned}
\end{equation}
and
\begin{equation}
    \begin{aligned}
        U^{(j)\dagger}J_-^{(j)}U^{(j)} &= U^{(j)\dagger}J_+^{(j)\dagger}U^{(j)} \\
        &= \left(U^{(j)\dagger}J_+^{(j)}U^{(j)}\right)^\dagger \\
        &= -J_-^{(j)\dagger} = -J_+^{(j)}.
    \end{aligned}
\end{equation}
This also directly implies
\begin{equation}
    \begin{aligned}
        U^{(j)\dagger}J_x^{(j)}U^{(j)} &= -J_x^{(j)} \\
        U^{(j)\dagger}J_y^{(j)}U^{(j)} &= J_y^{(j)} \\
        U^{(j)\dagger}J_z^{(j)}U^{(j)} &= -J_z^{(j)}
    \end{aligned}.
\end{equation}
Therefore,
\begin{equation}
    \begin{aligned}
        U^{(j)\dagger}D^{(j)}_{SU(2)\times P}(w)U^{(j)} &= \pm U^{(j)\dagger}\exp(i\vec{J}^{(j)}\cdot\vec{\phi})U^{(j)} \\ 
        &= \pm U^{(j)\dagger}\exp(i(J_x^{(j)}\phi_x+J_y^{(j)}\phi_y+J_z^{(j)}\phi_z))U^{(j)} \\
        &= \pm\exp(-i(J_x^{(j)}\phi_x-J_y^{(j)}\phi_y+J_z^{(j)}\phi_z)) \\
        &= D^{(j)}_{SU(2)\times P}(w)^\ast \\
        &= \tilde{D}^{(j)}_{SU(2)\times P}(w).
    \end{aligned}
\end{equation}
This directly means that the $SU(2)\times P$'s irreducible representation in $\{\ket{j, m, p}\}$, and $\{\theta\ket{j, m, p}\}$ basis are equivalent. This implies that the corepresentation of $SU(2)\times P\times TR$ for $w\in SU(2)\times P$ is
\begin{equation}
    D_{SU(2)\times P\times TR}^{(j)}(w) = 
    \begin{bmatrix}
        D^{(j)}_{SU(2)\times P}(w) & 0 \\
        0 & U^{(j)\dagger}D^{(j)}_{SU(2)\times P}(w)U^{(j)}
    \end{bmatrix}
\end{equation}
where the first half of the basis is $\{\ket{j, m, p}\}$ while the second half is $\{\theta\ket{j, m, p}\}$.

In order to get the corepresentation for $\theta$, we know that, since $\theta^{-1}w\theta$, $\theta^{-2}$ and $\theta^{2}$ are unitary,
\begin{equation}
    \tilde{D}^{(j)}_{SU(2)\times P}(\theta^{-1}w\theta) = D^{(j)}_{SU(2)\times P}(\theta^{-2}w\theta^2)^\ast.
\end{equation}
However, this exact thing is also equal
\begin{equation}
    \tilde{D}^{(j)}_{SU(2)\times P}(\theta^{-1}w\theta) = U^{(j)\dagger}D^{(j)}_{SU(2)\times P}(\theta^{-1}w\theta)U^{(j)}.
\end{equation}
Hence,
\begin{equation}
    \begin{aligned}
        D^{(j)}_{SU(2)\times P}(\theta^{-2}w\theta^2) &= U^{(j)\dagger\ast}D^{(j)}_{SU(2)\times P}(\theta^{-1}w\theta)^\ast U^{(j)\ast} \\
        D^{(j)}_{SU(2)\times P}(\theta^{2})^{-1}D^{(j)}_{SU(2)\times P}(w)D^{(j)}_{SU(2)\times P}(\theta^2)&= U^{(j)\dagger\ast}U^{(j)\dagger}D^{(j)}_{SU(2)\times P}(w)U^{(j)}U^{(j)\ast} \\
        U^{(j)}U^{(j)\ast}D^{(j)}_{SU(2)\times P}(\theta^{2})^{-1}D^{(j)}_{SU(2)\times P}(w) &= D^{(j)}_{SU(2)\times P}(w)U^{(j)}U^{(j)\ast}D^{(j)}_{SU(2)\times P}(\theta^2)^{-1}.
    \end{aligned}
\end{equation}
From Schur's lemma, since $D^{(j)}_{SU(2)\times P}$ is an irreducible representation, there is a constant $\lambda\in\mathbb{C}$ such that
\begin{equation}
    \lambda U^{(j)}U^{(j)\ast} = D^{(j)}_{SU(2)\times P}(\theta^{2}).
\end{equation}
With similar process but substitute $w=\theta^2$,
\begin{equation}
    D^{(j)}_{SU(2)\times P}(\theta^{2})^\ast = U^{(j)\dagger}D^{(j)}_{SU(2)\times P}(\theta^{2})U^{(j)}.
\end{equation}
This, together with the previous one, gives
\begin{equation}
    \lambda^\ast U^{(j)}U^{(j)\ast} = D^{(j)}_{SU(2)\times P}(\theta^{2}).
\end{equation}
Hence, $\lambda\in\mathbb{R}$. Moreover, the unitarity of $U^{(j)}$, and $SU(2)\times P$'s irreducible representation, restricts $\lambda = \pm 1$. With all these results, one now can write the corepresentation of $SU(2)\times P\times TR$ for $\theta\in T$ as
\begin{equation}
    \begin{aligned}
        D_{SU(2)\times P\times TR}^{(j)}(\theta) &= 
        \begin{bmatrix}
            0 & D^{(j)}_{SU(2)\times P}(\theta^2) \\
            1 & 0
        \end{bmatrix} \\
        &= \begin{bmatrix}
            0 & \lambda U^{(j)}U^{(j)\ast} \\
            1 & 0
        \end{bmatrix}.
    \end{aligned}
\end{equation}
One can further simplify the representation by changing the basis $\{\theta\ket{j, m, p}\}$ with $U^{(j)}$,
\begin{equation}
    \begin{aligned}
        D_{SU(2)\times P\times TR}^{(j)}(w) &= 
        \begin{bmatrix}
            D^{(j)}_{SU(2)\times P}(w) & 0 \\
            0 & D^{(j)}_{SU(2)\times P}(w)
        \end{bmatrix} \\
        D_{SU(2)\times P\times TR}^{(j)}(\theta) &= 
        \begin{bmatrix}
            0 & \lambda U^{(j)} \\
            U^{(j)} & 0
        \end{bmatrix}.
    \end{aligned}
\end{equation}

From definition of $U^{(j)}$, the representation $D_{SU(2)\times P\times TR}^{(j)}(\theta)$ can either be symmetric or antisymmetric antidiagonal matrix. We can see that
\begin{equation}
    \begin{aligned}
        \left[U^{(j)}\right]_{rq} &= i^{2(r-j)}\delta_{r,2j-q} \\
        &= i^{2(j-q)}\delta_{q,2j-r} \\
        &= i^{4(j-q)}\left[U^{(j)}\right]_{qr} \\
        &= (-1)^{2j}\left[U^{(j)}\right]_{qr}.
    \end{aligned}
\end{equation}
In other words, $D_{SU(2)\times P\times TR}^{(j)}(\theta)$ is symmetric when $(-1)^{2j}\lambda = 1$, and antisymmetric when $(-1)^{2j}\lambda = -1$. 

\subsection{Implementation}


\end{document}